::::: {.content-visible when-meta="diagnostics"}

### Setup

This [Quarto notebook](https://quarto.org/) establishes default parameters
in this section and utilizes the MultiQC Python package to locate log files
from tools executed within the workflow.

```{python}
#| tags: [parameters]

log_path = 'log_files'
inspect = True
preprocess = True
assemble = True
screen = True
purge = True
polish = True
scaffold = True
curate = True
alignRNA = False
diagnostics = False
```

#### MultiQC

```{python}
import glob
import multiqc
import os
import pandas as pd
from itertools import groupby
from pprint import pprint
from IPython.display import display # MQCPlot.show() returns a HTML object which needs to be forced to render in a cell
from IPython.display import Image

multiqc.reset() # Important for Quarto preview
# Load a custom config
multiqc.load_config(f"{log_path}/multiqc_assembly_report_config.yml")
multiqc.parse_logs(log_path)

# pprint pretty prints the dictionary
pprint(multiqc.list_plots())
pprint(multiqc.list_modules())

# Write out report (If other parts error, we should at least have the summary)
multiqc.write_report(
    force=True,
    output_dir=".",
)
```

#### Define utility functions

```{python}
#| echo: true
def displayTsvAsTable(glob_path):
    # Get a list of all TSV files in the directory
    tsvs = glob.glob(glob_path)

    # Read and concatenate the files
    list_of_dfs = []
    for filename in tsvs:
        df = pd.read_csv(filename, sep='\t')
        list_of_dfs.append(df)

    combined_df = pd.concat(list_of_dfs, ignore_index=True)
    # Transpose the combined DataFrame
    transposed_df = combined_df.set_index('file').transpose().reset_index()
    transposed_df.rename(columns={'index': 'metric'}, inplace=True)

    # Convert the combined DataFrame to a markdown table
    markdown_table = transposed_df.to_markdown(index=False)
    print("##### Seqkit stats\n")
    print(markdown_table)

def displayGenomeScopePlots():
    print("#### GenomeScope2 / GeneScopeFK\n")
    images = [
        img
        for pattern in ["log_plot", "linear_plot"]
        for img in glob.glob(f"{log_path}/*_{pattern}.png")
    ]

    # Ensure we have multiples of 4 images
    if len(images) % 4 != 0:
        raise ValueError("There should be 4 images per GenomeScope2 process")

    # Function to sort filenames
    def sort_key(filename):
        filename = os.path.basename(filename)
        parts = filename.split("_")
        # Order: linear < log, non-transformed < transformed
        order = {"linear": 0, "log": 1}
        transformed = 1 if "transformed" in parts else 0
        return (" ".join(parts[:2]), transformed, order[parts[-2]])

    # Sort filenames first
    sorted_filenames = sorted(images, key=sort_key)

    # Group by sample
    grouped_filenames = {}
    for key, group in groupby(
        sorted_filenames, key=lambda x: " ".join(os.path.basename(x).split("_")[:2])
    ):
        grouped_filenames[key] = list(group)

    # Function to format filenames
    def format_filename(filename):
        # return os.path.basename(filename).replace(".png", "").replace("_", " ")
        return " ".join(os.path.basename(filename).split("_")[2:-1])

    # Print grouped and sorted filenames
    tabs = []  # tabbed panels
    for sample, files in grouped_filenames.items():
        # tabs.append(f"### {sample}")
        tabs.append("::: {.panel-tabset}")
        for image in files:
            formatted_filename = format_filename(image)
            tabs.append(f"## {formatted_filename}")
            tabs.append(f"![{formatted_filename}]({image})")
        tabs.append(":::")

    markdown = "\n\n".join(tabs)
    print(markdown)

def displayAssemblyCompleteness(glob_path):
    print("#### Merqury Assembly k-mer completeness\n")
    print(
        pd.read_csv(
            glob.glob(glob_path)[0], sep="\t"
        ).to_markdown(index=False)
    )
    print("\n")

def displayAssemblyQualityValue(glob_path):
    print("#### Merqury Assembly quality value\n")
    print(
        pd.read_csv(
            glob.glob(glob_path)[0], sep="\t"
        ).to_markdown(index=False)
    )

def displayScaffoldQualityValue(glob_path):
    qvs = []
    for qv_file in glob.glob(glob_path):
        if os.path.getsize(qv_file) > 0:
            qvs.append(
                pd.read_csv(
                    qv_file,
                    sep="\t",
                    header=None,
                    names=[
                        "Scaffold",
                        "No support k-mers",
                        "Total k-mers",
                        "QV",
                        "Error rate",
                    ],
                    dtype={
                        "Scaffold": str,
                        "No support k-mers": int,
                        "Total k-mers": int,
                        "QV": float,
                        "Error rate": float,
                    },
                    na_values=["", "inf"],
                )
            )
    if qvs:
        print("<details><summary>QV per scaffold</summary>")
        per_scaffold_qv_raw = pd.concat(qvs, ignore_index=True)
        print(per_scaffold_qv_raw.to_markdown(index=False))
        print("</details>")
    else:
        print("No per scaffold qv data")

def displayGfastatsAssemblySummary(glob_path):
    print("#### GFAstats assembly summary")

    # Read the TSV file
    raw_asm_stats = pd.read_csv(
        glob.glob(glob_path)[0],
        sep=": ",
        engine='python',
        skiprows=1,
        header=None,
        names=["Metric", "Value"]
    )

    raw_asm_t1 = raw_asm_stats.iloc[[31, 32]].copy()
    print(raw_asm_t1.to_markdown(index=False))
    print("\n")

    # Table 2: Rows 1-11
    raw_asm_t2 = raw_asm_stats.iloc[1:12].copy()
    raw_asm_t2['Value'] = pd.to_numeric(raw_asm_t2['Value'], errors='coerce').astype(int)
    print(raw_asm_t2.to_markdown(index=False))
    print("\n")

    print("<details><summary>Contig and Gap stats</summary>\n")
    # Table 3: Rows 12-22
    raw_asm_t3 = raw_asm_stats.iloc[12:23].copy()
    raw_asm_t3['Value'] = pd.to_numeric(raw_asm_t3['Value'], errors='coerce').astype(int)
    print(raw_asm_t3.to_markdown(index=False))
    print("\n")

    # Table 4: Rows 24-30
    raw_asm_t4 = raw_asm_stats.iloc[24:31].copy()
    raw_asm_t4['Value'] = pd.to_numeric(raw_asm_t4['Value'], errors='coerce').astype(int)
    print(raw_asm_t4.to_markdown(index=False))
    print("\n")
    print("</details>")

def displayMerquryCopyNumberSpectra(merquryfk_cn_glob_path, merqury_cn_glob_path):
    print("#### Merqury Copy number spectra\n")
    print("\n:::: {.panel-tabset}\n")
    print("## MerquryFK\n")
    if glob.glob(merquryfk_cn_glob_path):
        display(Image(filename=glob.glob(merquryfk_cn_glob_path)[0]))
    else:
        print("MerquryFK copy number plot not available")
    print("\n## Merqury\n")
    if glob.glob(merqury_cn_glob_path):
        display(Image(filename=glob.glob(merqury_cn_glob_path)[0]))
    else:
        print("Merqury copy number plot not available")
    print("\n::::\n")

def displayMerquryAssemblySpectra(merquryfk_asm_glob_path, merqury_asm_glob_path):
    print("#### Merqury Assembly spectra\n")
    print("\n:::: {.panel-tabset}\n")
    print("## MerquryFK\n")
    if glob.glob(merquryfk_asm_glob_path):
        display(Image(filename=glob.glob(merquryfk_asm_glob_path)[0]))
    else:
        print("MerquryFK assembly spectra not available")
    print("\n## Merqury\n")
    if glob.glob(merqury_asm_glob_path):
        display(Image(filename=glob.glob(merqury_asm_glob_path)[0]))
    else:
        print("Merqury assembly spectra plot not available")
    print("\n::::\n")

def displayMerquryFalseDuplicationEstimate(glob_path):
    print("#### Merqury False duplication estimate")
    print(
        pd.read_csv(
            glob.glob(glob_path)[0], sep="\t"
        ).to_markdown(index=False)
    )

def displayFcsgxContaminationReport(glob_path):
    contamination_report = glob.glob(glob_path)[0]
    try:
        contamination_table = pd.read_csv(
            contamination_report,
            sep="\t",
            skiprows=2,
            header=None,
            names=['Seq ID', 'Start', 'End', 'Sequence Length', 'Action', 'Division', 'Coverage', 'Top Tax Name']
        )
    except (FileNotFoundError, ValueError) as e:
        raise ValueError(f"Unable to read contamination from '{contamination_report}': {e}") from e

    if not contamination_table.empty:
        print(contamination_table.to_markdown(index=False))
    else:
        print("No contamination found")

def displayPurgeDupsBinPercentages(glob_path):
    assembly_summary_file = glob.glob(glob_path)[0]
    try:
        # Read the assembly summary to get the assembly size
        # Use 'header=None' and 'names' to directly assign column names
        raw_asm_stats = pd.read_csv(
            assembly_summary_file,
            sep=": ",
            engine='python',
            skiprows=3,
            nrows=1,
            header=None,
            names=['key', 'value']
        )
        # Convert the 'value' to an integer
        assembly_size_bp = int(raw_asm_stats['value'].iloc[0])
    except (FileNotFoundError, IndexError, ValueError) as e:
        raise ValueError(f"Unable to determine assembly size from '{assembly_summary_file}': {e}") from e
    if assembly_size_bp <= 0:
        raise ValueError(f"Invalid assembly size found: {assembly_size_bp}")

    # Use a single DataFrame to aggregate results from all BED files
    all_bed_data = pd.DataFrame()
    for bed_file in glob.glob(f"{log_path}/*-purged-*.dups.bed"):
        try:
            dup_bed = pd.read_csv(
                bed_file,
                sep="\t",
                header=None,
                names=['Contig', 'Start', 'End', 'Type', 'Partner']
            )
            dup_bed['Length'] = dup_bed['End'] - dup_bed['Start']
            all_bed_data = pd.concat([all_bed_data, dup_bed], ignore_index=True)
        except pd.errors.EmptyDataError:
            continue # continue to next iteration of loop
        except Exception as e:
            print(f"Error processing {bed_file}: {e}")

    # Check if any data was processed
    if not all_bed_data.empty:
        # Group by 'Type' and sum 'Length' for all processed files
        summary = all_bed_data.groupby('Type')['Length'].sum().reset_index()
        summary['Percentage'] = (summary['Length'] * 100 / assembly_size_bp).round(2).astype(str) + '%'
        # Print the final summary in Markdown format
        print("#### Purged content")
        print(summary.to_markdown(index=False))
    else:
        print("Unable to calculate purged content percentage.")

def displayDvpolishSummary(dvpolish_selection_tsv):
    print("#### DeepVariant Polishing Summary\n")

    filepath = glob.glob(dvpolish_selection_tsv)[0]
    try:
        df = pd.read_csv(filepath, sep='\t')
    except Exception as e:
        print(f"Error reading {filepath}: {e}")
        return
    if df.empty:
        print("DeepVariant polishing selection file is empty.")
        return

    # Replace +inf with NaN for calculations (0 errors results in inf qv)
    df.replace([float('inf'), '+inf'], float('nan'), inplace=True)

    # Convert numeric columns to numeric, coercing errors
    numeric_cols = ['unpolished_asm_errors', 'polished_asm_errors', 'unpolished_asm_qv', 'polished_asm_qv']
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    total_contigs = len(df)

    # Source assembly counts
    source_counts = df['source_asm'].value_counts()
    unpolished_count = source_counts.get('unpolished', 0)
    polished_count = source_counts.get('polished', 0)

    # Compare error counts to see if polishing improved or degraded things
    error_diff = df['unpolished_asm_errors'] - df['polished_asm_errors']
    improved = (error_diff > 0).sum() # Unpolished had more errors than polished (polished selected)
    degraded = (error_diff < 0).sum() # Unpolished had fewer errors than polished (unpolished selected)
    unchanged = (error_diff == 0).sum() # Equal errors (unpolished selected)

    # Averages
    avg_unpol_errors = df['unpolished_asm_errors'].mean()
    avg_pol_errors = df['polished_asm_errors'].mean()
    avg_unpol_qv = df['unpolished_asm_qv'].mean()
    avg_pol_qv = df['polished_asm_qv'].mean()

    summary_data = {
            "Metric": [
                "Total contigs",
                "Selected from unpolished assembly",
                "Selected from polished assembly",
                "Contigs improved by polishing (errors reduced)",
                "Contigs degraded by polishing (errors increased)",
                "Contigs unchanged by polishing",
                "Avg unpolished errors",
                "Avg polished errors",
                "Avg unpolished QV",
                "Avg polished QV"
            ],
            "Value": [
                f"{total_contigs}",
                f"{unpolished_count} ({unpolished_count/total_contigs*100:.2f}%)" if total_contigs else "0",
                f"{polished_count} ({polished_count/total_contigs*100:.2f}%)" if total_contigs else "0",
                f"{improved} ({improved/total_contigs*100:.2f}%)" if total_contigs else "0",
                f"{degraded} ({degraded/total_contigs*100:.2f}%)" if total_contigs else "0",
                f"{unchanged} ({unchanged/total_contigs*100:.2f}%)" if total_contigs else "0",
                f"{avg_unpol_errors:.4f}",
                f"{avg_pol_errors:.4f}",
                f"{avg_unpol_qv:.4f}",
                f"{avg_pol_qv:.4f}"
            ]
    }

    summary_df = pd.DataFrame(summary_data)
    print(summary_df.to_markdown(index=False))
    print("\n")

```

:::::
